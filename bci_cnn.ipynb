{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from U:\\MEG-BCI\\main\\Deep Learning\\BCICIV_2b_gdf\\B0401T.gdf...\n",
      "GDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 604802  =      0.000 ...  2419.208 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sb00747428\\AppData\\Local\\Continuum\\anaconda2\\envs\\mne\\lib\\site-packages\\mne\\io\\edf\\edf.py:1083: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  etmode = np.fromstring(etmode, np.uint8).tolist()[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEG data set event information and index:\n",
      "  Event Type\tType Index\t                   Description\n",
      "         276\t         0\t        Idling EEG (eyes open)\n",
      "         277\t         1\t       Idling EEG (eyes closed\n",
      "         768\t         2\t              Start of a trial\n",
      "         769\t         3\t      Cue onset left (class 1)\n",
      "         770\t         4\t     Cue onset right (class 2)\n",
      "         781\t         5\t      BCI feedback (continuous\n",
      "         783\t         6\t                   Cue unknown\n",
      "        1023\t         7\t                Rejected trial\n",
      "        1077\t         8\t       Horizontal eye movement\n",
      "        1078\t         9\t         Vertical eye movement\n",
      "        1079\t        10\t                  Eye rotation\n",
      "        1081\t        11\t                    Eye blinks\n",
      "       32766\t        12\t            Start of a new run\n",
      "U:\\MEG-BCI\\main\\Deep Learning\\subject234\\B0401T.gdf\n",
      "Extracting EDF parameters from U:\\MEG-BCI\\main\\Deep Learning\\subject234\\B0401T.gdf...\n",
      "GDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 604802  =      0.000 ...  2419.208 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sb00747428\\AppData\\Local\\Continuum\\anaconda2\\envs\\mne\\lib\\site-packages\\mne\\io\\edf\\edf.py:1083: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  etmode = np.fromstring(etmode, np.uint8).tolist()[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U:\\MEG-BCI\\main\\Deep Learning\\subject234\\B0402T.gdf\n",
      "Extracting EDF parameters from U:\\MEG-BCI\\main\\Deep Learning\\subject234\\B0402T.gdf...\n",
      "GDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 696265  =      0.000 ...  2785.060 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sb00747428\\AppData\\Local\\Continuum\\anaconda2\\envs\\mne\\lib\\site-packages\\mne\\io\\edf\\edf.py:1083: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  etmode = np.fromstring(etmode, np.uint8).tolist()[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U:\\MEG-BCI\\main\\Deep Learning\\subject234\\B0403T.gdf\n",
      "Extracting EDF parameters from U:\\MEG-BCI\\main\\Deep Learning\\subject234\\B0403T.gdf...\n",
      "GDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 468558  =      0.000 ...  1874.232 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sb00747428\\AppData\\Local\\Continuum\\anaconda2\\envs\\mne\\lib\\site-packages\\mne\\io\\edf\\edf.py:1083: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  etmode = np.fromstring(etmode, np.uint8).tolist()[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U:\\MEG-BCI\\main\\Deep Learning\\subject234\\B0404E.gdf\n",
      "Extracting EDF parameters from U:\\MEG-BCI\\main\\Deep Learning\\subject234\\B0404E.gdf...\n",
      "GDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 467478  =      0.000 ...  1869.912 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sb00747428\\AppData\\Local\\Continuum\\anaconda2\\envs\\mne\\lib\\site-packages\\mne\\io\\edf\\edf.py:1083: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  etmode = np.fromstring(etmode, np.uint8).tolist()[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U:\\MEG-BCI\\main\\Deep Learning\\subject234\\B0405E.gdf\n",
      "Extracting EDF parameters from U:\\MEG-BCI\\main\\Deep Learning\\subject234\\B0405E.gdf...\n",
      "GDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 466050  =      0.000 ...  1864.200 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sb00747428\\AppData\\Local\\Continuum\\anaconda2\\envs\\mne\\lib\\site-packages\\mne\\io\\edf\\edf.py:1083: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  etmode = np.fromstring(etmode, np.uint8).tolist()[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U:\\MEG-BCI\\main\\Deep Learning\\subject234\\B0401T.mat\n",
      "U:\\MEG-BCI\\main\\Deep Learning\\subject234\\B0402T.mat\n",
      "U:\\MEG-BCI\\main\\Deep Learning\\subject234\\B0403T.mat\n",
      "U:\\MEG-BCI\\main\\Deep Learning\\subject234\\B0404E.mat\n",
      "U:\\MEG-BCI\\main\\Deep Learning\\subject234\\B0405E.mat\n",
      "test: read dataset\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "import os\n",
    "import re\n",
    "from scipy.io import loadmat\n",
    "\n",
    "\n",
    "# read raw EEG dataset of .gdf file\n",
    "class RawEEGData:\n",
    "    # rawData = list()\n",
    "    # event = pd.DataFrame()\n",
    "    # channel = list()\n",
    "    # sample_freq = 0\n",
    "\n",
    "    def __init__(self, file_name):\n",
    "        raw = mne.io.read_raw_edf(file_name, preload=True, stim_channel=-1)\n",
    "        event_type2idx = {276: 0, 277: 1, 768: 2, 769: 3, 770: 4, 781: 5, 783: 6, 1023: 7, 1077: 8, 1078: 9, 1079: 10,\n",
    "                          1081: 11, 32766: 12}\n",
    "        self.rawData = raw._data\n",
    "        self.channel = raw._raw_extras[0]['ch_names']\n",
    "        self.sample_freq = raw.info['sfreq']\n",
    "        # self.rawData =\n",
    "        self.event = pd.DataFrame({\n",
    "            \"length\":raw._raw_extras[0]['events'][0],\n",
    "            \"position\": raw._raw_extras[0]['events'][1],\n",
    "            \"event type\": raw._raw_extras[0]['events'][2],\n",
    "            \"event index\": [event_type2idx[event_type] for event_type in raw._raw_extras[0]['events'][2]],\n",
    "            \"duration\": raw._raw_extras[0]['events'][4],\n",
    "            \"CHN\": raw._raw_extras[0]['events'][3]\n",
    "        })\n",
    "\n",
    "    # print event type information of EEG data set\n",
    "    @staticmethod\n",
    "    def print_type_info():\n",
    "        print(\"EEG data set event information and index:\")\n",
    "        print(\"%12s\\t%10s\\t%30s\" % (\"Event Type\", \"Type Index\", \"Description\"))\n",
    "        print(\"%12d\\t%10d\\t%30s\" % (276, 0, \"Idling EEG (eyes open)\"))\n",
    "        print(\"%12d\\t%10d\\t%30s\" % (277, 1, \"Idling EEG (eyes closed\"))\n",
    "        print(\"%12d\\t%10d\\t%30s\" % (768, 2, \"Start of a trial\"))\n",
    "        print(\"%12d\\t%10d\\t%30s\" % (769, 3, \"Cue onset left (class 1)\"))\n",
    "        print(\"%12d\\t%10d\\t%30s\" % (770, 4, \"Cue onset right (class 2)\"))\n",
    "        print(\"%12d\\t%10d\\t%30s\" % (781, 5, \"BCI feedback (continuous\"))\n",
    "        print(\"%12d\\t%10d\\t%30s\" % (783, 6, \"Cue unknown\"))\n",
    "        print(\"%12d\\t%10d\\t%30s\" % (1023, 7, \"Rejected trial\"))\n",
    "        print(\"%12d\\t%10d\\t%30s\" % (1077, 8, \"Horizontal eye movement\"))\n",
    "        print(\"%12d\\t%10d\\t%30s\" % (1078, 9, \"Vertical eye movement\"))\n",
    "        print(\"%12d\\t%10d\\t%30s\" % (1079, 10, \"Eye rotation\"))\n",
    "        print(\"%12d\\t%10d\\t%30s\" % (1081, 11, \"Eye blinks\"))\n",
    "        print(\"%12d\\t%10d\\t%30s\" % (32766, 12, \"Start of a new run\"))\n",
    "\n",
    "\n",
    "# arrange data for training and test\n",
    "def get_data(data_file_dir, labels_file_dir):\n",
    "    RawEEGData.print_type_info()\n",
    "    sfreq = 250  # sample frequency of dataset\n",
    "    # read data file\n",
    "    data = dict()\n",
    "    data_files = os.listdir(data_file_dir)\n",
    "    for data_file in data_files:\n",
    "        if not re.search(\".*\\.gdf\", data_file):\n",
    "            continue\n",
    "        \n",
    "        info = re.findall('B0([0-9])0([0-9])[TE]\\.gdf', data_file)\n",
    "        try:\n",
    "            subject = \"subject\" + info[0][0]\n",
    "            session = \"session\" + info[0][1]\n",
    "            filename = data_file_dir + \"\\\\\" + data_file\n",
    "            print(filename)\n",
    "            raw_eeg_data = RawEEGData(filename)\n",
    "            trial_event = raw_eeg_data.event[raw_eeg_data.event['event index'] == 2]\n",
    "            session_data = dict()\n",
    "            for event, event_data in trial_event.iterrows():\n",
    "                trial_data = raw_eeg_data.rawData[:, event_data['position']:event_data['position']+event_data['duration']]\n",
    "                for idx in range(len(raw_eeg_data.channel)):\n",
    "                    if raw_eeg_data.channel[idx] not in session_data:\n",
    "                        session_data[raw_eeg_data.channel[idx]] = list()\n",
    "                    session_data[raw_eeg_data.channel[idx]].append(trial_data[idx])\n",
    "            if subject not in data:\n",
    "                data[subject] = dict()\n",
    "            data[subject][session] = session_data\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            raise (\"invalid data file name\")\n",
    "\n",
    "    # read data file\n",
    "    labels = dict()\n",
    "    labels_files = os.listdir(labels_file_dir)\n",
    "    for labels_file in labels_files:\n",
    "        if not re.search(\".*\\.mat\", labels_file):\n",
    "            continue\n",
    "\n",
    "        info = re.findall('B0([0-9])0([0-9])[TE]\\.mat', labels_file)\n",
    "        try:\n",
    "            subject = \"subject\" + info[0][0]\n",
    "            session = \"session\" + info[0][1]\n",
    "            filename = labels_file_dir + \"\\\\\" + labels_file\n",
    "            print(filename)\n",
    "            session_label = loadmat(filename)\n",
    "            session_label = session_label['classlabel'].astype(np.int8)\n",
    "            if subject not in labels:\n",
    "                labels[subject] = dict()\n",
    "            labels[subject][session] = session_label\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            raise (\"invalid labels file name\")\n",
    "\n",
    "    return data, labels, sfreq\n",
    "    # print(data)\n",
    "    # print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from scipy.signal import butter, lfilter, stft\n",
    "from scipy import interpolate\n",
    "import math\n",
    "from spectrum import pburg\n",
    "from sklearn import preprocessing\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "#from read_data import get_data\n",
    "\n",
    "\n",
    "# enrich data for each trail\n",
    "def preprocess_signal( ori_data, start_time, slide_len, segment_len, num, sfreq ):\n",
    "    processed_data = list()\n",
    "    for i in range(num):\n",
    "        left = int((start_time + i*slide_len)*sfreq)\n",
    "        right = left + sfreq*segment_len\n",
    "        # if need to be averaged\n",
    "        data_foo = ori_data[left:right]\n",
    "        data_foo = data_foo - np.mean(data_foo)\n",
    "\n",
    "        processed_data.append(data_foo)\n",
    "    return processed_data\n",
    "\n",
    "\n",
    "# butterworth band pass filter design\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "\n",
    "# butterworth band pass filter\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "\n",
    "# combine all zhe data of all trials, session\n",
    "def combine_processed_data(preprocessed_data, labels):\n",
    "    combined_data = dict()\n",
    "    combined_labels = dict()\n",
    "    for subject in preprocessed_data:\n",
    "        subject_data = pd.DataFrame()\n",
    "        subject_labels = list()\n",
    "        for session in preprocessed_data[subject]:\n",
    "            subject_data = subject_data.append(preprocessed_data[subject][session])\n",
    "            subject_labels.extend(labels[subject][session])\n",
    "        subject_combined_data = pd.DataFrame()\n",
    "        subject_combined_labels = list()\n",
    "        labels_flag = True\n",
    "        labels_idx = 0\n",
    "        for channel in subject_data:\n",
    "            channel_data = list()\n",
    "            for trials_data in subject_data[channel]:\n",
    "                for segment_data in trials_data:\n",
    "                    channel_data.append(segment_data)\n",
    "                    if labels_flag:\n",
    "                        subject_combined_labels.append(subject_labels[labels_idx])\n",
    "                labels_idx += 1\n",
    "            subject_combined_data[channel] = channel_data\n",
    "            labels_flag = False\n",
    "        combined_data[subject] = subject_combined_data\n",
    "        combined_labels[subject] = subject_combined_labels\n",
    "\n",
    "    return combined_data, combined_labels\n",
    "\n",
    "\n",
    "# subject optimal frequency bands selection methods based on Band Pass feature\n",
    "# type = 0, BP features\n",
    "# type = 1, AR features\n",
    "def feature_band_selection(data, labels, sfreq, step=1, band_range = (0, 0), band_size=(0, 0, 0),\n",
    "                              channel=('EEG:C3', 'EEG:Cz', 'EEG:C4'), features_type=0):\n",
    "    # AR model parameters\n",
    "    ar_order = 12\n",
    "    nfft = 1000\n",
    "\n",
    "    subject_optimal_frequency_bands = dict()\n",
    "\n",
    "    # mu band selection\n",
    "    for subject in data:\n",
    "        if features_type == 1: # compute AR Model PSD based on burg algorithm\n",
    "            ar_psd = dict()\n",
    "            freq_flag = True\n",
    "            for channel_name in channel:\n",
    "                ar_psd[channel_name] = list()\n",
    "            for idx in range(len(labels[subject])):\n",
    "                for channel_name in channel:\n",
    "                    x = data[subject][channel_name][idx]\n",
    "                    p = pburg(x, order=ar_order, NFFT=nfft, sampling=sfreq, scale_by_freq=True)\n",
    "                    if freq_flag:\n",
    "                        ar_psd['frequency'] = np.array(p.frequencies())\n",
    "                        freq_flag = False\n",
    "                    ar_psd[channel_name].append(p.psd)\n",
    "\n",
    "        f_score = list()\n",
    "        optimal_band = list()\n",
    "        for band in band_size:\n",
    "            for num_windows in range(int((band_range[1]-band_range[0]-band)/step)):\n",
    "                lowcut = band_range[0] + num_windows * step\n",
    "                highcut = lowcut + band\n",
    "                optimal_band.append((lowcut, highcut))\n",
    "                if features_type == 1:\n",
    "                    ar_freq = ar_psd['frequency']\n",
    "                    psd_idx_start = np.where(ar_freq >= lowcut)[0][0]\n",
    "                    psd_idx_end = np.where(ar_freq >= highcut)[0][0]\n",
    "                left_features = list()\n",
    "                right_features = list()\n",
    "                for idx in range(len(labels[subject])):\n",
    "                    features = list()\n",
    "                    for channel_name in channel:\n",
    "                        # BP features\n",
    "                        if features_type == 0: # 5th butterworth filter\n",
    "                            filtered_data = butter_bandpass_filter(data[subject][channel_name][idx], lowcut, highcut,\n",
    "                                                                   sfreq, order=5)\n",
    "                        elif features_type == 1: # AR model PSD\n",
    "                             filtered_data = ar_psd[channel_name][idx][psd_idx_start : psd_idx_end]\n",
    "                        else:\n",
    "                            raise Exception(\"feature type wrong!\\n band pass features: features_type=0\\n \"\n",
    "                                            \"AR PSD features: features_type=1\")\n",
    "                        features.append(math.log10(np.var(filtered_data)))\n",
    "                    if labels[subject][idx] == 1:\n",
    "                        left_features.append(features)\n",
    "                    elif labels[subject][idx] == 2:\n",
    "                        right_features.append(features)\n",
    "                left_mean_val = np.mean(left_features, axis=0)\n",
    "                right_mean_val = np.mean(right_features, axis=0)\n",
    "                left_var = np.var(left_features, axis=0)\n",
    "                right_var = np.var(right_features, axis=0)\n",
    "                f_score.append(sum(np.square(left_mean_val-right_mean_val)) / sum(left_var+right_var))\n",
    "        # get optimal frequency corresponding to max F-score\n",
    "        subject_optimal_frequency_bands[subject] = optimal_band[f_score.index(max(f_score))]\n",
    "        # pause = input(\"pause\")\n",
    "    return subject_optimal_frequency_bands\n",
    "\n",
    "# find kth largest number in a 1-d array\n",
    "def find_kth_largest(arr, k):\n",
    "    k = k - 1\n",
    "    lo = 0\n",
    "    hi = len(arr) - 1\n",
    "    while lo < hi:\n",
    "        arr[lo], arr[int((lo+hi)/2)] = arr[int((lo+hi)/2)], arr[lo]\n",
    "        left = lo; right = hi; pivot=arr[lo];\n",
    "        while left < right:\n",
    "            while left < right and arr[right] <= pivot:\n",
    "                right = right - 1\n",
    "            arr[left] = arr[right]\n",
    "            while left < right and arr[left] >= pivot:\n",
    "                left = left + 1\n",
    "            arr[right] = arr[left]\n",
    "        arr[left] = pivot\n",
    "        if k <= left:\n",
    "            hi = left - 1\n",
    "        if k >= left:\n",
    "            lo = left + 1\n",
    "    res = arr[k]\n",
    "    return res\n",
    "\n",
    "\n",
    "# rescale data\n",
    "# @percentage: percentage of value considered to be artifact\n",
    "def recale(data, percentage):\n",
    "    m, n = data.shape\n",
    "    arr = data.flatten()\n",
    "    min_val = np.min(arr)\n",
    "    max_val = find_kth_largest(arr, int(m*n*percentage))\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            if data[i][j] > max_val:\n",
    "                data[i][j] = 1\n",
    "            else:\n",
    "                data[i][j] = (data[i][j] - min_val) / ( max_val - min_val)\n",
    "    return data\n",
    "\n",
    "\n",
    "# get the input data of CNN by STFT\n",
    "def get_input_data(data, mu_band, beta_band, channel=('EEG:C3', 'EEG:Cz', 'EEG:C4')):\n",
    "    # parameters of stft:\n",
    "    wlen = 64  # length of the analysis Hamming window\n",
    "    nfft = 512  # number of FFT points\n",
    "    fs = 250  # sampling frequency, Hz\n",
    "    hop = 14  # hop size\n",
    "\n",
    "    input_data = list()\n",
    "    num_segments = len(data[channel[0]])\n",
    "    freq_flag = True\n",
    "    for idx in range(num_segments):\n",
    "        input_image = None\n",
    "        for chn in channel:\n",
    "            f, t, Fstft = stft(data[chn][idx], fs=fs, window='hamming', nperseg=wlen, noverlap=wlen-hop,\n",
    "                               nfft=nfft, return_onesided=True, boundary=None, padded=False)\n",
    "            if freq_flag:  # only need run one time\n",
    "                mu_left = np.where(f >= mu_band[0])[0][0]\n",
    "                mu_right = np.where(f >= mu_band[1])[0][0]\n",
    "                beta_left = np.where(f >= beta_band[0])[0][0]\n",
    "                beta_right = np.where(f >= beta_band[1])[0][0]\n",
    "                freq_flag = False\n",
    "            mu_feature_matrix = np.abs(Fstft[mu_left : mu_right])\n",
    "            beta_feature_matrix = np.abs(Fstft[beta_left : beta_right])\n",
    "\n",
    "            # beta band cubic interpolation\n",
    "            beta_interp = interpolate.interp2d(t, f[beta_left : beta_right], beta_feature_matrix, kind='cubic')\n",
    "            interNum = len(mu_feature_matrix)\n",
    "            f_beta = np.arange(beta_band[0], beta_band[1], (beta_band[1]-beta_band[0])/(interNum))\n",
    "            beta_feature_matrix = beta_interp(t, f_beta)\n",
    "            # mu_feature_matrix = preprocessing.scale(np.array(mu_feature_matrix), axis=1)\n",
    "            mu_feature_matrix = recale(mu_feature_matrix, 0.05)\n",
    "            beta_feature_matrix = recale(beta_feature_matrix, 0.05)\n",
    "            # mu_feature_matrix = preprocessing.scale(beta_feature_matrix, axis=1)\n",
    "            # plt.pcolormesh(t, f_beta, beta_feature_matrix, vmin=0)\n",
    "            # plt.show()\n",
    "            # pause = input(\"pause\")\n",
    "            if input_image is None:\n",
    "                input_image = np.append(mu_feature_matrix, beta_feature_matrix, axis=0)\n",
    "            else:\n",
    "                input_image = np.append(input_image, mu_feature_matrix, axis=0)\n",
    "                input_image = np.append(input_image, beta_feature_matrix, axis=0)\n",
    "        input_data.append(input_image)\n",
    "    return input_data\n",
    "\n",
    "\n",
    "\n",
    "# default run function\n",
    "# @band_type = 0: band pass optimal frequency bands\n",
    "# @band_type = 1: AR PSD optimal frequency bands\n",
    "# @band_type = 2: extend frequency band\n",
    "def run_sig_processing(data_src, labels_src, band_type):\n",
    "    # parameters initialization\n",
    "    start_time = 3\n",
    "    time_slides = 0.2\n",
    "    window_length = 2\n",
    "    segments_num = 11\n",
    "\n",
    "    data, labels, sfreq = get_data(data_src, labels_src)\n",
    "\n",
    "    # execute\n",
    "    preprocessed_data = dict()\n",
    "    for subject in data:\n",
    "        if subject not in preprocessed_data:\n",
    "            preprocessed_data[subject] = dict()\n",
    "        for session in data[subject]:\n",
    "            df_trials_data = pd.DataFrame()\n",
    "            for channel in data[subject][session]:\n",
    "                session_data = data[subject][session][channel]\n",
    "                trials_processed_data = list()\n",
    "                for trial_data in session_data:\n",
    "                    processed_data = preprocess_signal(trial_data, start_time, time_slides, window_length,\n",
    "                                                       segments_num, sfreq)\n",
    "                    trials_processed_data.append(processed_data)\n",
    "                df_trials_data[channel] = trials_processed_data\n",
    "            # print(df_trials_data)\n",
    "            # pause = input(\"pause: \")\n",
    "            preprocessed_data[subject][session] = df_trials_data\n",
    "\n",
    "    if band_type == 0 or band_type == 1:\n",
    "        combined_data, combined_labels = combine_processed_data(preprocessed_data, labels)\n",
    "        mu_band = feature_band_selection(combined_data, combined_labels, sfreq, step=1, band_range=(4, 14),\n",
    "                                          band_size=(4, 5, 6), features_type=band_type)\n",
    "        beta_band = feature_band_selection(combined_data, combined_labels, sfreq, step=1, band_range=(14, 32),\n",
    "                                          band_size=(4, 5, 6), features_type=band_type)\n",
    "    else:\n",
    "        mu_band = dict()\n",
    "        beta_band = dict()\n",
    "        for subject in preprocessed_data:\n",
    "            mu_band[subject] = (8, 12)\n",
    "            beta_band[subject] = (13, 30)\n",
    "\n",
    "    # get input data of CNN, add to column of dataFrame form processed_data[subject][session]\n",
    "    for subject in preprocessed_data:\n",
    "        for session in preprocessed_data[subject]:\n",
    "            preprocessed_data[subject][session]['input data'] \\\n",
    "                = preprocessed_data[subject][session].apply(get_input_data, axis=1,\n",
    "                                                            mu_band=mu_band[subject], beta_band=beta_band[subject])\n",
    "\n",
    "    return preprocessed_data, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ec81477fb1a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKFold\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda2\\envs\\mne\\lib\\site-packages\\sklearn\\model_selection\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_split\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_cv\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_validation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_validation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcross_val_predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_validation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcross_validate\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda2\\envs\\mne\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_joblib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogger\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexternals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscorer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_check_multimetric_scoring\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFitFailedWarning\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_split\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_cv\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda2\\envs\\mne\\lib\\site-packages\\sklearn\\metrics\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mranking\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mauc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mranking\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maverage_precision_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mranking\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcoverage_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda2\\envs\\mne\\lib\\site-packages\\sklearn\\metrics\\ranking.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrankdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0massert_all_finite\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda2\\envs\\mne\\lib\\site-packages\\scipy\\stats\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdivision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmorestats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda2\\envs\\mne\\lib\\site-packages\\scipy\\stats\\stats.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mspecial\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmstats_basic\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_distn_infrastructure\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_lazywhere\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda2\\envs\\mne\\lib\\site-packages\\scipy\\stats\\distributions.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdivision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m from ._distn_infrastructure import (entropy, rv_discrete, rv_continuous,\n\u001b[0m\u001b[0;32m     11\u001b[0m                                     rv_frozen)\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda2\\envs\\mne\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmisc\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdoccer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_distr_params\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistcont\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdistdiscrete\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_util\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_lazywhere\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_lazyselect\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda2\\envs\\mne\\lib\\site-packages\\scipy\\misc\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     62\u001b[0m            'comb', 'factorial', 'factorial2', 'factorialk', 'logsumexp']\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdoccer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwho\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_who\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_source\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda2\\envs\\mne\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda2\\envs\\mne\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda2\\envs\\mne\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda2\\envs\\mne\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda2\\envs\\mne\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda2\\envs\\mne\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from signalProcessing import run_sig_processing\n",
    "\n",
    "# get train data and labels for each segment\n",
    "def arrange_data(data, labels):\n",
    "    output_data = list()\n",
    "    output_labels = list()\n",
    "    for idx in range(len(data)):\n",
    "        for segment in data[idx]:\n",
    "            output_data.append(np.expand_dims(segment, axis=2))\n",
    "            if labels[idx][0] == 1:\n",
    "                output_labels.append(0)\n",
    "            else:\n",
    "                output_labels.append(1)\n",
    "    output_data = np.array(output_data)\n",
    "    output_labels = np.array(output_labels)\n",
    "    return output_data, output_labels\n",
    "\n",
    "\n",
    "# build model\n",
    "def build_model(size_y, size_x):\n",
    "    # input layer\n",
    "    img_input = keras.layers.Input(shape=(size_y, size_x, 1))\n",
    "    nf = size_y-2\n",
    "    x = keras.layers.Conv2D(filters= nf, kernel_size=(size_y, 3), activation='relu', \n",
    "                            kernel_regularizer=keras.regularizers.l2(0))(img_input)\n",
    "    x = keras.layers.MaxPooling2D(1, 10)(x)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    x = keras.layers.Dropout(0.5)(x)\n",
    "    output = keras.layers.Dense(2, activation='sigmoid')(x)\n",
    "    model = keras.models.Model(img_input, output)\n",
    "    model.summary()\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=keras.optimizers.SGD(lr=0.001, momentum=0.9),\n",
    "                  metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "# evaluated trial to trial performance\n",
    "def trial_evaluate(model, data, labels):\n",
    "    acc = 0.0\n",
    "    for idx in range(len(data)):\n",
    "        test_data, test_label = arrange_data(np.expand_dims(data[idx], axis=0), np.expand_dims(labels[idx], axis=0))\n",
    "        test_label = keras.utils.to_categorical(test_label, num_classes=2)\n",
    "        loss, accuracy = model.evaluate(test_data, test_label)\n",
    "        if accuracy > 0.5:\n",
    "            acc += 1.0\n",
    "    acc = acc/len(data)\n",
    "    return acc\n",
    "\n",
    "\n",
    "# run classification\n",
    "def run_classification(data, labels, session=(1, 2, 3, 4, 5)):\n",
    "    kf = KFold(n_splits=10, shuffle=True)\n",
    "    classification_acc = pd.DataFrame()\n",
    "    for subject in data:\n",
    "        # if subject == 'subject1': continue\n",
    "        subject_acc = list()\n",
    "        input_data = list()\n",
    "        target_labels = list()\n",
    "        # combine trials data of target session\n",
    "        [input_data.extend(data[subject][\"session\" + str(idx)]['input data']) for idx in session]\n",
    "        [target_labels.extend(labels[subject][\"session\" + str(idx)]) for idx in session]\n",
    "        input_data = np.array(input_data)\n",
    "        target_labels = np.array(target_labels)\n",
    "\n",
    "        # 10 fold cross-validation\n",
    "        count = 0\n",
    "        for train_index, test_index in kf.split(input_data):\n",
    "            count += 1\n",
    "            train_data, train_labels = arrange_data(input_data[train_index], target_labels[train_index])\n",
    "            test_data, test_labels = arrange_data(input_data[test_index], target_labels[test_index])\n",
    "\n",
    "            size_y, size_x = train_data[0].shape[0:2]\n",
    "\n",
    "            print(train_data.shape)\n",
    "            # train_data_size = train_data.shape[0]\n",
    "            # test_data_size = test_data.shape[0]\n",
    "\n",
    "            train_labels = keras.utils.to_categorical(train_labels, num_classes=2)\n",
    "            test_labels = keras.utils.to_categorical(test_labels, num_classes=2)\n",
    "\n",
    "\n",
    "            # build model\n",
    "            model = build_model(size_y, size_x)\n",
    "\n",
    "            print('Training ------------')\n",
    "            # train the model\n",
    "            history = model.fit(train_data, train_labels, validation_split=0.33, epochs=300, batch_size=40)\n",
    "            print(history.history.keys())\n",
    "            # summarize history for accuracy\n",
    "            plt.plot(history.history['acc'])\n",
    "            plt.plot(history.history['val_acc'])\n",
    "            plt.title('model accuracy')\n",
    "            plt.ylabel('accuracy')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.legend(['train', 'test'], loc='upper left')\n",
    "            plt.show()\n",
    "            # summarize history for loss\n",
    "            plt.plot(history.history['loss'])\n",
    "            plt.plot(history.history['val_loss'])\n",
    "            plt.title('model loss')\n",
    "            plt.ylabel('loss')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.legend(['train', 'test'], loc='upper left')\n",
    "            plt.show()\n",
    "\n",
    "            print('\\nTesting ------------')\n",
    "            # Evaluate the model with the metrics we defined earlier\n",
    "            loss, accuracy = model.evaluate(test_data, test_labels)\n",
    "\n",
    "            trial_acc = trial_evaluate(model, input_data[test_index], target_labels[test_index])\n",
    "            print(count, subject)\n",
    "            print('test loss: ', loss)\n",
    "            print('test accuracy: ', accuracy)\n",
    "            print('trial to trial accuracy: ', trial_acc)\n",
    "            subject_acc.append(trial_acc)\n",
    "        classification_acc[subject] = subject_acc\n",
    "    return classification_acc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # '''\n",
    "    data_src = r\"U:\\MEG-BCI\\main\\Deep Learning\\subject234\"\n",
    "    labels_src = r\"U:\\MEG-BCI\\main\\Deep Learning\\subject234\"\n",
    "    data, labels = run_sig_processing(data_src, labels_src, band_type=3)\n",
    "\n",
    "    # Saving the data and labels:\n",
    "    # with open('temp_data.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "    #     pickle.dump([data, labels], f)\n",
    "    # '''\n",
    "    # Getting back the data and labels:\n",
    "    # with open('temp_data.pkl', 'rb') as f:  # Python 3: open(..., 'rb')\n",
    "    #     data, labels = pickle.load(f)\n",
    "    res = run_classification(data, labels)\n",
    "    print(res)\n",
    "    res.to_csv(\"BP_acc2_beta_gamma.csv\", encoding=\"utf-8\")\n",
    "    print(\"cnn classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
